"""File classification for Lean files - detect definition vs proof files and sorries."""

import re
from dataclasses import dataclass
from pathlib import Path

from git_history_proof_engineering_eval.structures import SorryLocation


@dataclass
class FileClassification:
    """Classification results for a Lean file."""

    is_definition: bool
    is_proof: bool
    contains_aeneas: bool
    theorem_count: int
    lemma_count: int
    tactic_block_count: int


def is_aeneas_generated(content: str) -> bool:
    """Check if a file is Aeneas-generated.

    Args:
        content: File content as string.

    Returns:
        True if file has Aeneas header or imports from Curve25519Dalek.Funs.
    """
    # Check for Aeneas header
    if "AUTOMATICALLY GENERATED BY AENEAS" in content:
        return True

    # Check for imports from Aeneas-generated modules
    if re.search(r"import\s+Curve25519Dalek\.Funs", content):
        return True

    return False


def count_tactic_blocks(content: str) -> int:
    """Count significant tactic blocks (by := ...).

    Args:
        content: File content as string.

    Returns:
        Number of tactic blocks found.
    """
    # Match "by" keyword followed by tactics
    # This is a simple heuristic - real tactic blocks may span multiple lines
    pattern = r"\bby\s+"
    return len(re.findall(pattern, content))


def classify_file(file_path: Path) -> FileClassification:
    """Classify a Lean file as definition, proof, or both.

    Classification rules:
    - Definition files: contain 'inductive', 'structure', 'def', or Aeneas header
    - Proof files: contain 'theorem', 'lemma', or significant tactic blocks

    Args:
        file_path: Path to Lean file.

    Returns:
        FileClassification with analysis results.
    """
    try:
        content = file_path.read_text(encoding="utf-8")
    except Exception:
        # If we can't read the file, classify as neither
        return FileClassification(
            is_definition=False,
            is_proof=False,
            contains_aeneas=False,
            theorem_count=0,
            lemma_count=0,
            tactic_block_count=0,
        )

    # Check for Aeneas-generated content
    contains_aeneas = is_aeneas_generated(content)

    # Count proof constructs
    theorem_count = len(re.findall(r"\btheorem\s+", content))
    lemma_count = len(re.findall(r"\blemma\s+", content))
    tactic_block_count = count_tactic_blocks(content)

    # Count definition constructs
    def_count = len(re.findall(r"\bdef\s+", content))
    inductive_count = len(re.findall(r"\binductive\s+", content))
    structure_count = len(re.findall(r"\bstructure\s+", content))

    # Classify based on heuristics
    is_definition = (
        contains_aeneas or def_count > 0 or inductive_count > 0 or structure_count > 0
    )

    is_proof = theorem_count > 0 or lemma_count > 0 or tactic_block_count >= 3

    return FileClassification(
        is_definition=is_definition,
        is_proof=is_proof,
        contains_aeneas=contains_aeneas,
        theorem_count=theorem_count,
        lemma_count=lemma_count,
        tactic_block_count=tactic_block_count,
    )


def should_exclude_path(file_path: Path, exclude_patterns: list[str]) -> bool:
    """Check if a file path matches any exclude patterns.

    Args:
        file_path: Path to check.
        exclude_patterns: List of path patterns to exclude (e.g., "Utils/", ".github/").

    Returns:
        True if file should be excluded.
    """
    path_str = str(file_path)
    return any(pattern in path_str for pattern in exclude_patterns)


def find_sorries(content: str) -> list[SorryLocation]:
    """Find all sorry/admit locations in content.

    Args:
        content: Lean file content as string.

    Returns:
        List of SorryLocation objects for each sorry found.
    """
    sorries = []
    lines = content.split("\n")
    current_decl: str | None = None

    for i, line in enumerate(lines, 1):
        # Track enclosing declaration (theorem, lemma, or def)
        decl_match = re.search(r"\b(theorem|lemma|def)\s+(\w+)", line)
        if decl_match:
            current_decl = decl_match.group(2)

        # Find sorries on this line
        for match in re.finditer(r"\b(sorry|admit)\b", line):
            sorries.append(
                SorryLocation(
                    line=i,
                    column=match.start(),
                    context=line.strip(),
                    enclosing_decl=current_decl,
                )
            )

    return sorries


def find_filled_sorries(parent_content: str, child_content: str) -> list[SorryLocation]:
    """Find sorries in parent that were filled in child (by declaration context).

    Args:
        parent_content: Lean file content at parent commit.
        child_content: Lean file content at child commit.

    Returns:
        List of SorryLocation objects from parent where the enclosing declaration
        no longer has sorries in child.
    """
    parent_sorries = find_sorries(parent_content)
    child_sorries = find_sorries(child_content)

    # Group by enclosing declaration
    child_decl_sorries = {s.enclosing_decl for s in child_sorries if s.enclosing_decl}
    parent_decl_sorries = {s.enclosing_decl for s in parent_sorries if s.enclosing_decl}

    # Sorries are "filled" if the declaration no longer has sorries in child
    filled_decls = parent_decl_sorries - child_decl_sorries

    return [s for s in parent_sorries if s.enclosing_decl in filled_decls]
